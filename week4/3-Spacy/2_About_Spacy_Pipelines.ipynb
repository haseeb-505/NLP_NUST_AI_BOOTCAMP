{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Processing pipelines\n",
        "\n",
        "This notebook is about Spacy pipeline feature. A pipeline is a series of functions applied to a doc to add attributes like part-of-speech tags, dependency labels or named entities."
      ],
      "metadata": {
        "id": "P4hNuGBzxpK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What does spaCy do when you call nlp on a string of text?\n",
        "\n",
        "doc = nlp(\"this is a test sentence.\")"
      ],
      "metadata": {
        "id": "NOqT8GsYx_S_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspecting the pipeline\n",
        "\n",
        "Let’s inspect the small English pipeline!\n",
        "\n",
        "* Print the names of the pipeline components using nlp.pipe_names.\n",
        "* Print the full pipeline of (name, component) tuples using nlp.pipeline."
      ],
      "metadata": {
        "id": "to-lmglkyNZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the en_core_web_sm pipeline\n",
        "nlp = ____\n",
        "\n",
        "# Print the names of the pipeline components\n",
        "print(____.____)\n",
        "\n",
        "# Print the full pipeline of (name, component) tuples\n",
        "print(____.____)"
      ],
      "metadata": {
        "id": "q19q9CBdyWmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the en_core_web_sm pipeline\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Print the names of the pipeline components\n",
        "print(nlp.pipe_names)\n",
        "\n",
        "# Print the full pipeline of (name, component) tuples\n",
        "print(nlp.pipeline)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqQCKwpYyZQO",
        "outputId": "5ef74bd5-3649-448e-97d7-33efc8a8ff66"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
            "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7ae28fe2fca0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7ae28fe2fbe0>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7ae34064f450>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7ae28fdbbb00>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7ae2900d6b40>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7ae340cbaff0>)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custome pipelilne components\n",
        "\n",
        "Now that you know how spaCy's pipeline works, let's take a look at another very powerful feature: custom pipeline components.\n",
        "\n",
        "Custom pipeline components let you add your own function to the spaCy pipeline that is executed when you call the nlp object on a text – for example, to modify the doc and add more data to it.\n",
        "\n"
      ],
      "metadata": {
        "id": "-3vrIb9byjBM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple components\n",
        "\n",
        "The example shows a custom component that prints the number of tokens in a document. Can you complete it?\n",
        "\n",
        "* Complete the component function with the doc’s length.\n",
        "* Add the \"length_component\" to the existing pipeline as the first component.\n",
        "* Try out the new pipeline and process any text with the nlp object – for example “This is a sentence.”."
      ],
      "metadata": {
        "id": "WwiA-rzQy1kn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.language import Language\n",
        "\n",
        "# Define the custom component\n",
        "@Language.component(\"length_component\")\n",
        "def length_component_function(doc):\n",
        "    # Get the doc's length\n",
        "    doc_length = ____\n",
        "    print(f\"This document is {doc_length} tokens long.\")\n",
        "    # Return the doc\n",
        "    ____\n",
        "\n",
        "\n",
        "# Load the small English pipeline\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Add the component first in the pipeline and print the pipe names\n",
        "____.____(____, ____=____)\n",
        "print(nlp.pipe_names)\n",
        "\n",
        "# Process a text\n",
        "doc = ____"
      ],
      "metadata": {
        "id": "C8VKpssSzS_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HzkeT0cxXqB"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.language import Language\n",
        "\n",
        "# Define the custom component\n",
        "@Language.component(\"length_component\")\n",
        "def length_component_function(doc):\n",
        "    # Get the doc's length\n",
        "    doc_length = len(doc)\n",
        "    print(f\"This document is {doc_length} tokens long.\")\n",
        "    # Return the doc\n",
        "    return doc\n",
        "\n",
        "\n",
        "# Load the small English pipeline\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Add the component first in the pipeline and print the pipe names\n",
        "nlp.add_pipe(\"length_component\", first=True)\n",
        "print(nlp.pipe_names)\n",
        "\n",
        "# Process a text\n",
        "doc = nlp(\"This is a sentence.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complex components\n",
        "\n",
        "In this exercise, you’ll be writing a custom component that uses the PhraseMatcher to find animal names in the document and adds the matched spans to the doc.ents. A PhraseMatcher with the animal patterns has already been created as the variable matcher.\n",
        "\n",
        "* Define the custom component and apply the matcher to the doc.\n",
        "* Create a Span for each match, assign the label ID for \"ANIMAL\" and overwrite the doc.ents with the new spans.\n",
        "* Add the new component to the pipeline after the \"ner\" component.\n",
        "* Process the text and print the entity text and entity label for the entities in doc.ents."
      ],
      "metadata": {
        "id": "7X_7hzK3zaLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.language import Language\n",
        "from spacy.matcher import PhraseMatcher\n",
        "from spacy.tokens import Span\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
        "animal_patterns = list(nlp.pipe(animals))\n",
        "print(\"animal_patterns:\", animal_patterns)\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "matcher.add(\"ANIMAL\", animal_patterns)\n",
        "\n",
        "# Define the custom component\n",
        "@Language.component(\"animal_component\")\n",
        "def animal_component_function(doc):\n",
        "    # Apply the matcher to the doc\n",
        "    matches = ____\n",
        "    # Create a Span for each match and assign the label \"ANIMAL\"\n",
        "    spans = [Span(____, ____, ___, label=____) for match_id, start, end in matches]\n",
        "    # Overwrite the doc.ents with the matched spans\n",
        "    doc.ents = spans\n",
        "    return doc\n"
      ],
      "metadata": {
        "id": "7Dt0Cc_WzmuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the component to the pipeline after the \"ner\" component\n",
        "____.____(____, ____=____)\n",
        "print(nlp.pipe_names)\n",
        "\n",
        "# Process the text and print the text and label for the doc.ents\n",
        "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
        "print([(____, ____) for ent in ____])"
      ],
      "metadata": {
        "id": "BfNbvv8zzqy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.language import Language\n",
        "from spacy.matcher import PhraseMatcher\n",
        "from spacy.tokens import Span\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
        "animal_patterns = list(nlp.pipe(animals))\n",
        "print(\"animal_patterns:\", animal_patterns)\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "matcher.add(\"ANIMAL\", animal_patterns)\n",
        "\n",
        "# Define the custom component\n",
        "@Language.component(\"animal_component\")\n",
        "def animal_component_function(doc):\n",
        "    # Apply the matcher to the doc\n",
        "    matches = matcher(doc)\n",
        "    # Create a Span for each match and assign the label \"ANIMAL\"\n",
        "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
        "    # Overwrite the doc.ents with the matched spans\n",
        "    doc.ents = spans\n",
        "    return doc\n",
        "\n",
        "\n",
        "# Add the component to the pipeline after the \"ner\" component\n",
        "nlp.add_pipe(\"animal_component\", after=\"ner\")\n",
        "print(nlp.pipe_names)\n",
        "\n",
        "# Process the text and print the text and label for the doc.ents\n",
        "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asvNz7k7ztvJ",
        "outputId": "228b2c9f-45be-4316-90f5-075a9173e507"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "animal_patterns: [Golden Retriever, cat, turtle, Rattus norvegicus]\n",
            "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'animal_component']\n",
            "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extension attributes\n",
        "\n",
        "In this lesson, you'll learn how to add custom attributes to the Doc, Token and Span objects to store custom data."
      ],
      "metadata": {
        "id": "zpdoHAVR0Iph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1\n",
        "\n",
        "* Use Token.set_extension to register \"is_country\" (default False).\n",
        "* Update it for \"Pakistan\" and print it for all tokens."
      ],
      "metadata": {
        "id": "otXZEeLl0XAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import Token\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Register the Token extension attribute \"is_country\" with the default value False\n",
        "____.____(____, ____=____)\n",
        "\n",
        "# Process the text and set the is_country attribute to True for the token \"Pakistan\"\n",
        "doc = nlp(\"I live in Pakistan.\")\n",
        "____ = True\n",
        "\n",
        "# Print the token text and the is_country attribute for all tokens\n",
        "print([(____, ____) for token in doc])"
      ],
      "metadata": {
        "id": "yMU9UUrs0eqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import Token\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Register the Token extension attribute \"is_country\" with the default value False\n",
        "Token.set_extension(\"is_country\", default=False)\n",
        "\n",
        "# Process the text and set the is_country attribute to True for the token \"Pakistan\"\n",
        "doc = nlp(\"I live in Pakistan.\")\n",
        "doc[3]._.is_country = True\n",
        "\n",
        "# Print the token text and the is_country attribute for all tokens\n",
        "print([(token.text, token._.is_country) for token in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLS8k6-s0iwr",
        "outputId": "d17b6ace-0210-4737-b954-bb048dbade2a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', False), ('live', False), ('in', False), ('Pakistan', True), ('.', False)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2\n",
        "* Use Token.set_extension to register \"reversed\" (getter function get_reversed).\n",
        "* Print its value for each token."
      ],
      "metadata": {
        "id": "cKao-mC4xiFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import Token\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Define the getter function that takes a token and returns its reversed text\n",
        "def get_reversed(token):\n",
        "    return token.text[::-1]\n",
        "\n",
        "\n",
        "# Register the Token property extension \"reversed\" with the getter get_reversed\n",
        "____.____(____, ____=____)\n",
        "\n",
        "# Process the text and print the reversed attribute for each token\n",
        "doc = nlp(\"All generalizations are false, including this one.\")\n",
        "for ____ in ____:\n",
        "    print(\"reversed:\", ____)"
      ],
      "metadata": {
        "id": "3RY9IOhY1RWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import Token\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Define the getter function that takes a token and returns its reversed text\n",
        "def get_reversed(token):\n",
        "    return token.text[::-1]\n",
        "\n",
        "\n",
        "# Register the Token property extension \"reversed\" with the getter get_reversed\n",
        "Token.set_extension(\"reversed\", getter=get_reversed)\n",
        "\n",
        "# Process the text and print the reversed attribute for each token\n",
        "doc = nlp(\"All generalizations are false, including this one.\")\n",
        "for token in doc:\n",
        "    print(\"reversed:\", token._.reversed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tumvw8Q1U3D",
        "outputId": "90f213c3-ae2e-4ccd-aada-51caf8d9b512"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reversed: llA\n",
            "reversed: snoitazilareneg\n",
            "reversed: era\n",
            "reversed: eslaf\n",
            "reversed: ,\n",
            "reversed: gnidulcni\n",
            "reversed: siht\n",
            "reversed: eno\n",
            "reversed: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting extension attributes - Complex Case\n",
        "\n",
        "Let’s try setting some more complex attributes using getters and method extensions.\n",
        "\n",
        "## Part 1\n",
        "* Complete the get_has_number function .\n",
        "* Use Doc.set_extension to register \"has_number\" (getter get_has_number) and print its value.\n",
        "\n"
      ],
      "metadata": {
        "id": "1GvW8XFt1iyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Define the getter function\n",
        "def get_has_number(doc):\n",
        "    # Return if any of the tokens in the doc return True for token.like_num\n",
        "    return any(____ for token in doc)\n",
        "\n",
        "\n",
        "# Register the Doc property extension \"has_number\" with the getter get_has_number\n",
        "____.____(____, ____=____)\n",
        "\n",
        "# Process the text and check the custom has_number attribute\n",
        "doc = nlp(\"The museum closed for five years in 2024.\")\n",
        "print(\"has_number:\", ____)"
      ],
      "metadata": {
        "id": "kGMFHcOa1yfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Define the getter function\n",
        "def get_has_number(doc):\n",
        "    # Return if any of the tokens in the doc return True for token.like_num\n",
        "    return any(token.like_num for token in doc)\n",
        "\n",
        "\n",
        "# Register the Doc property extension \"has_number\" with the getter get_has_number\n",
        "Doc.set_extension(\"has_number\", getter=get_has_number)\n",
        "\n",
        "# Process the text and check the custom has_number attribute\n",
        "doc = nlp(\"The museum closed for five years in 2024.\")\n",
        "print(\"has_number:\", doc._.has_number)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lpRh4LP122U",
        "outputId": "3216d02b-f082-4ad1-ebb5-1bcb21f5b01b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "has_number: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2\n",
        "* Use Span.set_extension to register \"to_html\" (method to_html).\n",
        "* Call it on doc[0:2] with the tag \"strong\"."
      ],
      "metadata": {
        "id": "aQuJW1VG19Ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import Span\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Define the method\n",
        "def to_html(span, tag):\n",
        "    # Wrap the span text in a HTML tag and return it\n",
        "    return f\"<{tag}>{span.text}</{tag}>\"\n",
        "\n",
        "\n",
        "# Register the Span method extension \"to_html\" with the method to_html\n",
        "____.____(____, ____=____)\n",
        "\n",
        "# Process the text and call the to_html method on the span with the tag name \"strong\"\n",
        "doc = nlp(\"Hello world, this is a sentence.\")\n",
        "span = doc[0:2]\n",
        "print(____)"
      ],
      "metadata": {
        "id": "Z3yEYYzy2BkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import Span\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Define the method\n",
        "def to_html(span, tag):\n",
        "    # Wrap the span text in a HTML tag and return it\n",
        "    return f\"<{tag}>{span.text}</{tag}>\"\n",
        "\n",
        "\n",
        "# Register the Span method extension \"to_html\" with the method to_html\n",
        "Span.set_extension(\"to_html\", method=to_html)\n",
        "\n",
        "# Process the text and call the to_html method on the span with the tag name \"strong\"\n",
        "doc = nlp(\"Hello world, this is a sentence.\")\n",
        "span = doc[0:2]\n",
        "print(span._.to_html(\"strong\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzp6QLKK2DoC",
        "outputId": "7a370fa8-8b32-41e2-8edd-54b05397fe01"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<strong>Hello world</strong>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entities and extension\n",
        "\n",
        "In this exercise, you’ll combine custom extension attributes with the statistical predictions and create an attribute getter that returns a Wikipedia search URL if the span is a person, organization, or location.\n",
        "\n",
        "* Complete the get_wikipedia_url getter so it only returns the URL if the span’s label is in the list of labels.\n",
        "* Set the Span extension \"wikipedia_url\" using the getter get_wikipedia_url.\n",
        "* Iterate over the entities in the doc and output their Wikipedia URL."
      ],
      "metadata": {
        "id": "yp7dxzyH2M9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import Span\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "def get_wikipedia_url(span):\n",
        "    # Get a Wikipedia URL if the span has one of the labels\n",
        "    if ____ in (\"PERSON\", \"ORG\", \"GPE\", \"LOCATION\"):\n",
        "        entity_text = span.text.replace(\" \", \"_\")\n",
        "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
        "\n",
        "\n",
        "# Set the Span extension wikipedia_url using the getter get_wikipedia_url\n",
        "____.____(____, ____=____)\n",
        "\n",
        "doc = nlp(\n",
        "    \"In over fifty years from his very first recordings right through to his \"\n",
        "    \"last album, David Bowie was at the vanguard of contemporary culture.\"\n",
        ")\n",
        "for ent in doc.ents:\n",
        "    # Print the text and Wikipedia URL of the entity\n",
        "    print(____, ____)"
      ],
      "metadata": {
        "id": "2uYGoYdk2WNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import Span\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "def get_wikipedia_url(span):\n",
        "    # Get a Wikipedia URL if the span has one of the labels\n",
        "    if span.label_ in (\"PERSON\", \"ORG\", \"GPE\", \"LOCATION\"):\n",
        "        entity_text = span.text.replace(\" \", \"_\")\n",
        "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
        "\n",
        "\n",
        "# Set the Span extension wikipedia_url using the getter get_wikipedia_url\n",
        "Span.set_extension(\"wikipedia_url\", getter=get_wikipedia_url)\n",
        "\n",
        "doc = nlp(\n",
        "    \"In over fifty years from his very first recordings right through to his \"\n",
        "    \"last album, David Bowie was at the vanguard of contemporary culture.\"\n",
        ")\n",
        "for ent in doc.ents:\n",
        "    # Print the text and Wikipedia URL of the entity\n",
        "    print(ent.text, ent._.wikipedia_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kG0x-UXj2kkI",
        "outputId": "d5afd662-01ba-4852-ee4d-8b99992e1b5e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fifty years None\n",
            "first None\n",
            "David Bowie https://en.wikipedia.org/w/index.php?search=David_Bowie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Components with extensions\n",
        "\n",
        "Extension attributes are especially powerful if they’re combined with custom pipeline components. In this exercise, you’ll write a pipeline component that finds country names and a custom extension attribute that returns a country’s capital, if available.\n",
        "\n",
        "A phrase matcher with all countries is available as the variable matcher. A dictionary of countries mapped to their capital cities is available as the variable CAPITALS.\n",
        "\n",
        "* Complete the countries_component_function and create a Span with the label \"GPE\" (geopolitical entity) for all matches.\n",
        "* Add the component to the pipeline.\n",
        "* Register the Span extension attribute \"capital\" with the getter get_capital.\n",
        "* Process the text and print the entity text, entity label and entity capital for each entity span in doc.ents."
      ],
      "metadata": {
        "id": "CCZxVrZz2rsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import spacy\n",
        "from spacy.language import Language\n",
        "from spacy.tokens import Span\n",
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "with open(\"countries.json\", encoding=\"utf8\") as f:\n",
        "    COUNTRIES = json.loads(f.read())\n",
        "\n",
        "with open(\"capitals.json\", encoding=\"utf8\") as f:\n",
        "    CAPITALS = json.loads(f.read())\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "matcher.add(\"COUNTRY\", list(nlp.pipe(COUNTRIES)))\n",
        "\n",
        "\n",
        "@Language.component(\"countries_component\")\n",
        "def countries_component_function(doc):\n",
        "    # Create an entity Span with the label \"GPE\" for all matches\n",
        "    matches = matcher(doc)\n",
        "    doc.ents = [Span(doc, start, end, label=\"GPE\") for match_id, start, end in matches]\n",
        "    return doc\n",
        "\n",
        "\n",
        "# Add the component to the pipeline\n",
        "nlp.add_pipe(\"countries_component\")\n",
        "print(nlp.pipe_names)\n",
        "\n",
        "# Getter that looks up the span text in the dictionary of country capitals\n",
        "get_capital = lambda span: CAPITALS.get(span.text)\n",
        "\n",
        "# Register the Span extension attribute \"capital\" with the getter get_capital\n",
        "Span.set_extension(\"capital\", getter=get_capital)\n",
        "\n",
        "# Process the text and print the entity text, label and capital attributes\n",
        "\n",
        "doc = nlp(\"Pakistan may help China to expand its global out-reach\")\n",
        "\n",
        "print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IGL1-hD-2348",
        "outputId": "a8be8a78-0e98-4e1d-d0a5-a30c34981812"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['countries_component']\n",
            "[('Pakistan', 'GPE', 'Islamabad'), ('China', 'GPE', 'Beijing')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing stream\n",
        "\n",
        "In this exercise, you’ll be using nlp.pipe for more efficient text processing. The nlp object has already been created for you. A list of tweets about a popular American fast food chain are available as the variable TEXTS."
      ],
      "metadata": {
        "id": "TcY6kD7s5b_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1\n",
        "* Rewrite the example to use nlp.pipe. Instead of iterating over the texts and processing them, iterate over the doc objects yielded by nlp.pipe."
      ],
      "metadata": {
        "id": "ssan68dW5jeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "with open(\"exercises/en/tweets.json\", encoding=\"utf8\") as f:\n",
        "    TEXTS = json.loads(f.read())\n",
        "\n",
        "# Process the texts and print the adjectives\n",
        "for text in TEXTS:\n",
        "    doc = nlp(text)\n",
        "    print([token.text for token in doc if token.pos_ == \"ADJ\"])"
      ],
      "metadata": {
        "id": "kGElw13n5nL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "with open(\"tweets.json\", encoding=\"utf8\") as f:\n",
        "    TEXTS = json.loads(f.read())\n",
        "\n",
        "# Process the texts and print the adjectives\n",
        "for doc in nlp.pipe(TEXTS):\n",
        "    print([token.text for token in doc if token.pos_ == \"ADJ\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrL7I_vp5iDt",
        "outputId": "6002a168-1e04-4f72-eebf-20738b522f2f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['favorite']\n",
            "['sick']\n",
            "[]\n",
            "['happy']\n",
            "['delicious', 'fast']\n",
            "[]\n",
            "['terrible']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2\n",
        "* Rewrite the example to use nlp.pipe. Don’t forget to call list() around the result to turn it into a list."
      ],
      "metadata": {
        "id": "FyH0SuOA644P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "with open(\"tweets.json\", encoding=\"utf8\") as f:\n",
        "    TEXTS = json.loads(f.read())\n",
        "\n",
        "# Process the texts and print the entities\n",
        "docs = [nlp(text) for text in TEXTS]\n",
        "entities = [doc.ents for doc in docs]\n",
        "print(*entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdZuD5Sa69DS",
        "outputId": "f3721baa-7778-43d9-a14d-946521705849"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(McDonalds,) () (McDonalds,) (McDonalds, Spain) (The Arch Deluxe,) () (This morning,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selective processing\n",
        "\n",
        "In this exercise, you’ll use the nlp.make_doc and nlp.select_pipes methods to only run selected components when processing a text.\n",
        "\n",
        "## Part 1\n",
        "* Rewrite the code to only tokenize the text using nlp.make_doc."
      ],
      "metadata": {
        "id": "jbxe6_pf78A8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = (\n",
        "    \"Chick-fil-A is an American fast food restaurant chain headquartered in \"\n",
        "    \"the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
        ")\n",
        "\n",
        "# Only tokenize the text\n",
        "doc = nlp.make_doc(text)\n",
        "print([token.text for token in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoFDZitM8FwX",
        "outputId": "2232fce1-17f9-4a58-b6df-bc98442019d2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Chick', '-', 'fil', '-', 'A', 'is', 'an', 'American', 'fast', 'food', 'restaurant', 'chain', 'headquartered', 'in', 'the', 'city', 'of', 'College', 'Park', ',', 'Georgia', ',', 'specializing', 'in', 'chicken', 'sandwiches', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2\n",
        "* Disable the tagger and lemmatizer using the nlp.select_pipes method.\n",
        "Process the text and print all entities in the doc."
      ],
      "metadata": {
        "id": "SQlqgNOC8MaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = (\n",
        "    \"Chick-fil-A is an American fast food restaurant chain headquartered in \"\n",
        "    \"the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
        ")\n",
        "\n",
        "# Disable the tagger and lemmatizer\n",
        "with nlp.select_pipes(disable=[\"tagger\", \"lemmatizer\"]):\n",
        "    # Process the text\n",
        "    doc = nlp(text)\n",
        "    # Print the entities in the doc\n",
        "    print(doc.ents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUfhT6MU8Py9",
        "outputId": "05ba76d5-d2ba-43df-97bd-c5e8aa4a9c60"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Chick, American, College Park, Georgia)\n"
          ]
        }
      ]
    }
  ]
}